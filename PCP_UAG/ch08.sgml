<!-- Fragment document type declaration subset:
ArborText, Inc., 1988-1997, v.4001
<!DOCTYPE SGIDOCBK PUBLIC "-//Silicon Graphics, Inc.//DTD DocBook V2.3-based Subset V1.5//EN" [
<!ENTITY disclaimer.sgml SYSTEM "frontmatter/disclaimer.sgml">
<!ENTITY o3000 SYSTEM "online/o3000.gif" NDATA gif>
<!ENTITY figure5y SYSTEM "online/figure5y.gif" NDATA gif>
<!ENTITY figure5x SYSTEM "online/figure5x.gif" NDATA gif>
<!ENTITY mpivis SYSTEM "online/mpivis.gif" NDATA gif>
<!ENTITY oview-tool.sgml SYSTEM "oview-tool.sgml">
<!ENTITY glossary.sgml SYSTEM "glossary.sgml">
<!ENTITY chap09.misc.sgml SYSTEM "chap09.misc.sgml">
<!ENTITY chap08.pixie.sgml SYSTEM "chap08.pixie.sgml">
<!ENTITY chap07.prof.sgml SYSTEM "chap07.prof.sgml">
<!ENTITY chap06.ssrun.sgml SYSTEM "chap06.ssrun.sgml">
<!ENTITY chap05.machuse.sgml SYSTEM "chap05.machuse.sgml">
<!ENTITY chap04.exptypes.sgml SYSTEM "chap04.exptypes.sgml">
<!ENTITY chap03.ftut.sgml SYSTEM "chap03.ftut.sgml">
<!ENTITY chap02.ctut.sgml SYSTEM "chap02.ctut.sgml">
<!ENTITY chap01.intro.sgml SYSTEM "chap01.intro.sgml">
<!ENTITY whentouseidmap.sgml SYSTEM "whentouseidmap.sgml">
<!ENTITY nfsintro.sgml SYSTEM "nfsintro.sgml">
<!ENTITY idmapping.sgml SYSTEM "idmapping.sgml">
<!ENTITY chapB.sgml SYSTEM "chapB.sgml">
<!ENTITY chapC.sgml SYSTEM "chapC.sgml">
<!ENTITY chapD.sgml SYSTEM "chapD.sgml">
<!ENTITY chapE.sgml SYSTEM "chapE.sgml">
<!ENTITY chapF.sgml SYSTEM "chapF.sgml">
<!ENTITY chapA.sgml SYSTEM "chapA.sgml">
<!ENTITY chap01.sgml SYSTEM "chap01.sgml">
<!ENTITY chap02.sgml SYSTEM "chap02.sgml">
<!ENTITY chap03.sgml SYSTEM "chap03.sgml">
<!ENTITY chap04.sgml SYSTEM "chap04.sgml">
<!ENTITY a12231 SYSTEM "online/a12231.gif" NDATA gif>
<!ENTITY a12129 SYSTEM "online/a12129.gif" NDATA gif>
<!ENTITY a11363 SYSTEM "online/a11363.gif" NDATA gif>
<!ENTITY a11362 SYSTEM "online/a11362.gif" NDATA gif>
<!ENTITY a10650 SYSTEM "online/a10650.gif" NDATA gif>
<!ENTITY a10209 SYSTEM "online/a10209.gif" NDATA gif>
<!ENTITY a10180 SYSTEM "online/a10180.gif" NDATA gif>
<!ENTITY a10181 SYSTEM "online/a10181.gif" NDATA gif>
<!ENTITY a10195 SYSTEM "online/a10195.gif" NDATA gif>
<!ENTITY a10196 SYSTEM "online/a10196.gif" NDATA gif>
<!ENTITY a10197 SYSTEM "online/a10197.gif" NDATA gif>
<!ENTITY a10198 SYSTEM "online/a10198.gif" NDATA gif>
<!ENTITY a10200 SYSTEM "online/a10200.gif" NDATA gif>
<!ENTITY a10201 SYSTEM "online/a10201.gif" NDATA gif>
<!ENTITY a10206 SYSTEM "online/a10206.gif" NDATA gif>
<!ENTITY a10207 SYSTEM "online/a10207.gif" NDATA gif>
<!ENTITY a10253 SYSTEM "online/a10253.gif" NDATA gif>
<!ENTITY a10254 SYSTEM "online/a10254.gif" NDATA gif>
<!ENTITY a10255 SYSTEM "online/a10255.gif" NDATA gif>
<!ENTITY a10256 SYSTEM "online/a10256.gif" NDATA gif>
<!ENTITY a10257 SYSTEM "online/a10257.gif" NDATA gif>
<!ENTITY a10258 SYSTEM "online/a10258.gif" NDATA gif>
<!ENTITY a10259 SYSTEM "online/a10259.gif" NDATA gif>
<!ENTITY a11427 SYSTEM "online/a11427.gif" NDATA gif>
<!ENTITY a11428 SYSTEM "online/a11428.gif" NDATA gif>
<!ENTITY a11429 SYSTEM "online/a11429.gif" NDATA gif>
<!ENTITY a11430 SYSTEM "online/a11430.gif" NDATA gif>
<!ENTITY a11431 SYSTEM "online/a11431.gif" NDATA gif>
<!ENTITY a11432 SYSTEM "online/a11432.gif" NDATA gif>
<!ENTITY conventions.abbreviation SYSTEM "frontmatter/conventions.abbreviation.sgml">
<!ENTITY collections SYSTEM "dwebcollections.sgml">
<!ENTITY unicos SYSTEM "frontmatter/unicos.sgml">
<!ENTITY trademarks SYSTEM "frontmatter/trademarks.sgml">
<!ENTITY disclaimer SYSTEM "frontmatter/disclaimer.sgml">
<!ENTITY rights SYSTEM "frontmatter/rights.sgml">
<!ENTITY manpage.section SYSTEM "frontmatter/manpage.section.sgml">
<!ENTITY % public.private "INCLUDE">
<!ENTITY % proprietary "IGNORE">
<!ENTITY % private "IGNORE">
<!ENTITY % public "INCLUDE">
<!ENTITY % craysoft "INCLUDE">
<!ENTITY conventions.ellipses SYSTEM "frontmatter/conventions.ellipses.sgml">
<!ENTITY conventions.manpage SYSTEM "frontmatter/conventions.manpage.sgml">
<!ENTITY conventions.variable SYSTEM "frontmatter/conventions.variable.sgml">
<!ENTITY conventions.brackets SYSTEM "frontmatter/conventions.brackets.sgml">
<!ENTITY conventions.command SYSTEM "frontmatter/conventions.command.sgml">
<!ENTITY conventions.userinput SYSTEM "frontmatter/conventions.userinput.sgml">
<!ENTITY machines.craympp SYSTEM "frontmatter/machines.craympp.sgml">
<!ENTITY machines.allcraysystems SYSTEM "frontmatter/machines.allcraysystems.sgml">
<!ENTITY standards SYSTEM "frontmatter/standards.sgml">
<!ENTITY reader.comments SYSTEM "frontmatter/reader.comments.sgml">
<!ENTITY ordering.pubs SYSTEM "frontmatter/ordering.pubs.sgml">
<!ENTITY a12229 SYSTEM "online/a12229.gif" NDATA gif>
<!ENTITY a12228 SYSTEM "online/a12228.gif" NDATA gif>
<!ENTITY a12227 SYSTEM "online/a12227.gif" NDATA gif>
<!ENTITY a12226 SYSTEM "online/a12226.gif" NDATA gif>
<!ENTITY a12225 SYSTEM "online/a12225.gif" NDATA gif>
<!ENTITY a12224 SYSTEM "online/a12224.gif" NDATA gif>
<!ENTITY a12223 SYSTEM "online/a12223.gif" NDATA gif>
<!ENTITY a12222 SYSTEM "online/a12222.gif" NDATA gif>
<!ENTITY a12221 SYSTEM "online/a12221.gif" NDATA gif>
<!ENTITY a12220 SYSTEM "online/a12220.gif" NDATA gif>
<!ENTITY a12219 SYSTEM "online/a12219.gif" NDATA gif>
<!ENTITY a12218 SYSTEM "online/a12218.gif" NDATA gif>
<!ENTITY a12217 SYSTEM "online/a12217.gif" NDATA gif>
<!ENTITY a12216 SYSTEM "online/a12216.gif" NDATA gif>
<!ENTITY a12215 SYSTEM "online/a12215.gif" NDATA gif>
<!ENTITY a12214 SYSTEM "online/a12214.gif" NDATA gif>
<!ENTITY a12213 SYSTEM "online/a12213.gif" NDATA gif>
<!ENTITY a12212 SYSTEM "online/a12212.gif" NDATA gif>
<!ENTITY a12211 SYSTEM "online/a12211.gif" NDATA gif>
<!ENTITY a12210 SYSTEM "online/a12210.gif" NDATA gif>
<!ENTITY a12209 SYSTEM "online/a12209.gif" NDATA gif>
<!ENTITY a12208 SYSTEM "online/a12208.gif" NDATA gif>
<!ENTITY a12207 SYSTEM "online/a12207.gif" NDATA gif>
<!ENTITY a12206 SYSTEM "online/a12206.gif" NDATA gif>
<!ENTITY a12205 SYSTEM "online/a12205.gif" NDATA gif>
<!ENTITY a12204 SYSTEM "online/a12204.gif" NDATA gif>
<!ENTITY a12203 SYSTEM "online/a12203.gif" NDATA gif>
<!ENTITY a12202 SYSTEM "online/a12202.gif" NDATA gif>
<!ENTITY a12201 SYSTEM "online/a12201.gif" NDATA gif>
<!ENTITY a12200 SYSTEM "online/a12200.gif" NDATA gif>
<!ENTITY a12199 SYSTEM "online/a12199.gif" NDATA gif>
<!ENTITY a12198 SYSTEM "online/a12198.gif" NDATA gif>
<!ENTITY a12197 SYSTEM "online/a12197.gif" NDATA gif>
<!ENTITY a12196 SYSTEM "online/a12196.gif" NDATA gif>
<!ENTITY a12195 SYSTEM "online/a12195.gif" NDATA gif>
<!ENTITY a12194 SYSTEM "online/a12194.gif" NDATA gif>
<!ENTITY a12193 SYSTEM "online/a12193.gif" NDATA gif>
<!ENTITY a12192 SYSTEM "online/a12192.gif" NDATA gif>
<!ENTITY a12191 SYSTEM "online/a12191.gif" NDATA gif>
<!ENTITY a12190 SYSTEM "online/a12190.gif" NDATA gif>
<!ENTITY a12189 SYSTEM "online/a12189.gif" NDATA gif>
<!ENTITY ch01.sgml SYSTEM "ch01.sgml">
<!ENTITY ch02.sgml SYSTEM "ch02.sgml">
<!ENTITY ch03.sgml SYSTEM "ch03.sgml">
<!ENTITY ch04.sgml SYSTEM "ch04.sgml">
<!ENTITY ch05.sgml SYSTEM "ch05.sgml">
<!ENTITY ch06.sgml SYSTEM "ch06.sgml">
<!ENTITY ch07.sgml SYSTEM "ch07.sgml">
<!ENTITY ch09.sgml SYSTEM "ch09.sgml">
<!ENTITY chA.sgml SYSTEM "chA.sgml">
<!ENTITY preface.sgml SYSTEM "preface.sgml">
]>
-->
<?Pub UDT _bookmark _target>
<?Pub UDT _nopagebreak _touchup KeepsKeep="yes" KeepsPrev="no" KeepsNext="no" KeepsBoundary="page">
<?Pub Inc>
<chapter id="LE83321-PARENT">
<title id="LE83321-TITLE">Performance Co-Pilot Deployment Strategies</title>
<para><indexterm><primary>deployment strategies</primary></indexterm>Performance
Co-Pilot (PCP) is a coordinated suite of tools and utilities allowing
you to monitor performance and make automated judgments and initiate actions
based on those judgments. PCP is designed to be fully configurable for
custom implementation and deployed to meet specific needs in a variety
of operational environments.</para>
<para>Because each enterprise and site is different and PCP represents
a new way of visualizing performance information, some discussion of deployment
strategies is useful.</para>
<para><indexterm><primary>PMCD</primary><secondary>monitoring utilities
</secondary></indexterm><indexterm><primary>PMDA</primary><secondary>
monitoring utilities</secondary></indexterm>The most common use of performance
monitoring utilities is a scenario where the PCP tools are executed on
a workstation (the PCP monitoring system), while the interesting performance
data is collected on remote systems (PCP collector systems) by a number
of processes, specifically the Performance Metrics Collection Daemon (PMCD)
and the associated Performance Metrics Domain Agents (PMDAs). These processes
can execute on both the monitoring system and one or more collector systems,
or only on collector systems. However, collector systems are the real
objects of performance investigations.</para>
<para>The material in this chapter covers the following areas:</para>
<itemizedlist>
<listitem><para><xref linkend="LE85282-PARENT">, presents the spectrum
of deployment architectures at the highest level.</para>
</listitem>
<listitem><para><xref linkend="LE69500-PARENT">, describes alternative
deployments for PMCD and the PMDAs.</para>
</listitem>
<listitem><para><xref linkend="LE56598-PARENT">, covers alternative deployments
for the <command>pmlogger</command> tool.</para>
</listitem>
<listitem><para><xref linkend="LE62310-PARENT">, presents the options
that are available for deploying the <command>pmie</command> tool.</para>
</listitem></itemizedlist>
<?Pub _newpage>
<para>The options shown in this chapter are merely suggestions. They are
not comprehensive, and are intended to demonstrate some possible ways
of deploying the PCP tools for specific network topologies and purposes.
You are encouraged to use them as the basis for planning your own deployment,
consistent with your needs.</para>
<section id="LE85282-PARENT">
<title id="LE85282-TITLE">Basic Deployment</title>
<para>In the simplest PCP deployment, one system is configured as both
a collector and a monitor, as shown in <xref linkend="LE73801-TITLE">.
Because the PCP monitor tools make extensive use of visualization, this
suggests the single system would be configured with a graphics head.</para>
<figure>
<graphic entityref="a12223" scale="NO"></graphic>
<title id="LE73801-TITLE">PCP Deployment for a Single System</title>
</figure>
<?Pub _newpage>
<para>However, most PCP deployments involve at least two systems. For
example, the setup shown in <xref linkend="LE21701-TITLE"> would be representative
of many common scenarios.</para>
<figure width="wide">
<graphic entityref="a12224" scale="NO"></graphic>
<title id="LE21701-TITLE">Basic PCP Deployment for Two Systems</title>
</figure>
<para>But the most common site configuration would include a mixture of
systems configured as PCP collectors, as PCP monitors, and as both PCP
monitors and collectors, as shown in <xref linkend="LE74419-TITLE">.</para>
<?Pub _newpage>
<para>With one or more PCP collector systems and one or more PCP monitor
systems, there are a number of decisions that need to be made regarding
the deployment of PCP services across multiple hosts. For example, in <xref
linkend="LE74419-TITLE"> there are several ways in which both the inference
engine (<command>pmie</command>) and the PCP archive logger (<command>
pmlogger</command>) could be deployed. These options are discussed in
the following sections of this chapter.</para>
<figure width="wide">
<graphic entityref="a12225" scale="NO"></graphic>
<title id="LE74419-TITLE">General PCP Deployment for Multiple Systems
</title>
</figure>
</section>
<?Pub _newpage>
<section id="LE69500-PARENT">
<title id="LE69500-TITLE">PCP Collector Deployment</title>
<para><indexterm id="ITch08-0"><primary>PCP</primary><secondary>collector
deployment</secondary></indexterm>Each PCP collector system must have
an active <literal>pmcd</literal> and, typically, a number of PMDAs installed.
</para>
<section>
<title>Principal Server Deployment</title>
<para>The first hosts selected as PCP collector systems are likely to
provide some class of service deemed to be critical to the information
processing activities of the enterprise. These hosts include the following:
</para>
<itemizedlist>
<listitem><para>A server running a DBMS</para>
</listitem>
<listitem><para>A World Wide Web server for an Internet or Intranet</para>
</listitem>
<listitem><para>An NFS file server</para>
</listitem>
<listitem><para>A video server</para>
</listitem>
<listitem><para>A supercomputing server</para>
</listitem>
<listitem><para>An infrastructure service provider, for example, print,
Usenet news, DNS, gateway, firewall, packet router, or mail services</para>
</listitem>
<listitem><para>A system running a mission-critical application</para>
</listitem></itemizedlist>
<para>Your objective may be to improve quality of service on a system
functioning as a server for many clients. You wish to identify and repair
critical performance bottlenecks and deficiencies in order to maintain
maximum performance for clients of the server.</para>
<para>For some of these services, the PCP base product or the PCP add-on
products provide the necessary collector components. Others would require
customized PMDA development, as described in the companion <string strname="007-3434"><citetitle>
Performance Co-Pilot Programmer's Guide</citetitle></string>.</para>
</section>
<?Pub _newpage>
<section>
<title>Quality of Service Measurement</title>
<para><indexterm><primary>service management</primary></indexterm>Applications
and services with a client-server architecture need to monitor performance
at both the server side and the client side.</para>
<para>The arrangement in <xref linkend="LE63100-TITLE"> illustrates one
way of measuring quality of service for client-server applications.</para>
<figure width="wide">
<graphic entityref="a12226" scale="NO"></graphic>
<title id="LE63100-TITLE">PCP Deployment to Measure Client-Server Quality
of Service</title>
</figure>
<para>The configuration of the PCP collector components on the Application
Server System is standard. The new facility is the deployment of some
PCP collector components on the Application Client System; this uses a
customized PMDA and a generalization of the ICMP &ldquo;ping&rdquo; tool
as follows:</para>
<itemizedlist>
<listitem><para>The <literal>Client App</literal> is specially developed
to periodically make typical requests of the <literal>App Server</literal>,
and to measure the response time for these requests (this is an application-specific &ldquo;ping&rdquo;).
</para>
</listitem>
<listitem><para>The PMDA on the Application Client System captures the
response time measurements from the <literal>Client App</literal> and
exports these into the PCP framework.</para>
</listitem></itemizedlist>
<para>At the PCP monitor system, the performance of the system running
the <literal>App Server</literal> and the end-user quality of service
measurements from the system where the <literal>Client App</literal> is
running can be monitored concurrently.</para>
<para>PCP add-on products implement a number of examples of this architecture,
including the <literal>shping</literal> PMDA for IP-based services and
the <literal>webping</literal> PMDA for Web servers.</para>
<para>For each of these PMDAs, the full source code is distributed with
the associated PCP product to encourage adaptation of the agents to the
local application environment.</para>
<para>It is possible to exploit this arrangement even further, with these
methods:</para>
<itemizedlist>
<listitem><para>Creating new instances of the <literal>Client App</literal>
and PMDA to measure service quality for your own mission-critical services.
</para>
</listitem>
<listitem><para>Deploying the <literal>Client App</literal> and associated
PCP collector components in a number of strategic hosts allows the quality
of service over the enterprise's network to be monitored. For example,
service can be monitored on the Application Server System, on the same
LAN segment as the Application Server System, on the other side of a firewall
system, or out in the WAN.</para>
</listitem></itemizedlist>
</section>
</section>
<section id="LE56598-PARENT">
<title id="LE56598-TITLE">PCP Archive Logger Deployment</title>
<para><indexterm id="ITch08-3"><primary>PCP</primary><secondary>archive
logger deployment</secondary></indexterm>PCP archive logs are created
by the <command>pmlogger</command> utility, as discussed in <xref linkend="LE93354-TITLE">.
They provide a critical capability to perform retrospective performance
analysis, for example, to detect performance regressions, for problem
analysis, or to support capacity planning. The following sections discuss
the options and trade-offs for <command>pmlogger</command> deployment.
</para>
<section>
<title>Deployment Options</title>
<para>The issue is relatively simple and reduces to &ldquo;On which host(s)
should <command>pmlogger</command> be running?&rdquo; The options are
these:</para>
<itemizedlist>
<listitem><para>Run <command>pmlogger</command> on each PCP collector
system to capture local performance data.</para>
</listitem>
<listitem><para>Run <command>pmlogger</command> on some of the PCP monitor
systems to capture performance data from remote PCP collector systems.
</para>
</listitem>
<listitem><para>As an extension of the previous option, designate one
system to act as the PCP archive site to run all <command>pmlogger</command>
instances. This arrangement is shown in <xref linkend="LE23570-TITLE">.
</para>
<figure width="wide">
<graphic entityref="a12227" scale="NO"></graphic>
<title id="LE23570-TITLE">Designated PCP Archive Site</title>
</figure>
</listitem></itemizedlist>
</section>
<section>
<title>Resource Demands for the Deployment Options</title>
<para>The <command>pmlogger</command> process is very lightweight in terms
of computational demand; so most of the (small) CPU cost associated with
extracting performance metrics at the PCP collector system involves PMCD
and the PMDAs, which are independent of the host on which <command>pmlogger
</command> is running.</para>
<para>A local <command>pmlogger</command> consumes disk bandwidth and
disk space on the PCP collector system. A remote <command>pmlogger</command>
consumes disk space on the site where it is running and network bandwidth
between that host and the PCP collector host.</para>
<para>The archive logs typically grow at the rate of between 500 kilobytes
(KB) and 10 megabytes (MB) per day, depending on how many performance
metrics are logged and the choice of sampling frequencies. There are some
advantages in minimizing the number of hosts over which the disk resources
for PCP archive logs must be allocated; however, the aggregate requirement
is independent of where the <command>pmlogger</command> instances are
running.</para>
</section>
<section>
<title>Operational Management</title>
<para>There is an initial administrative cost associated with configuring
each <command>pmlogger</command> instance, and an ongoing administrative
investment to monitor these configurations, perform regular housekeeping
(such as rotation, compression, and culling of PCP archive log files),
and execute periodic tasks to process the archives (such as nightly performance
regression checking with <command>pmie</command>)<?Pub Caret1>.</para>
<para>Many of these tasks are handled by the supplied <command>pmlogger
</command> administrative tools and scripts, as described in <xref linkend="LE92914-TITLE">.
However, the necessity and importance of these tasks favor a centralized <command>
pmlogger</command> deployment, as shown in <xref linkend="LE23570-TITLE">.
</para>
<note><para>The <command>pmlogger</command> utility is not subject to
any PCP license restrictions, and may be installed and used on any host.
</para>
</note>
</section>
<section>
<title>Exporting PCP Archive Logs</title>
<para><indexterm><primary>archive logs</primary><secondary>export</secondary>
</indexterm>Collecting PCP archive logs is of little value unless the
logs are processed as part of the ongoing performance monitoring and management
functions. This processing typically involves the use of the tools on
a PCP monitor system, and hence the archive logs may need to be read on
a host different from the one they were created on.</para>
<para>NFS mounting is obviously an option, but the PCP tools support random
access and both forward and backward temporal motion within an archive
log. If an archive is to be subjected to intensive and interactive processing,
it may be more efficient to copy the files of the archive log to the PCP
monitor system first.</para>
<note><para>Each PCP archive log consists of at least three separate files
(see <xref linkend="LE92914-TITLE"> for details). You must have concurrent
access to all of these files before a PCP tool is able to process an archive
log correctly.</para>
</note>
</section>
</section>
<?Pub _newpage>
<section id="LE62310-PARENT">
<title id="LE62310-TITLE">PCP Inference Engine Deployment</title>
<para>The <command>pmie</command> utility supports automated reasoning
about system performance, as discussed in <xref linkend="LE21414-TITLE">,
and plays a key role in monitoring system performance for both real-time
and retrospective analysis, with the performance data being retrieved
respectively from a PCP collector system and a PCP archive log.</para>
<para>The following sections discuss the options and trade-offs for <command>
pmie</command> deployment.</para>
<section>
<title>Deployment Options</title>
<para>The issue is relatively simple and reduces to &ldquo;On which host(s)
should <command>pmie</command> be running?&rdquo; You must consider both
real-time and retrospective uses, and the options are as follows:</para>
<itemizedlist>
<listitem><para>For real-time analysis, run <command>pmie</command> on
each PCP collector system to monitor local system performance.</para>
</listitem>
<listitem><para>For real-time analysis, run <command>pmie</command> on
some of the PCP monitor systems to monitor the performance of remote PCP
collector systems.</para>
</listitem>
<listitem><para>For retrospective analysis, run <command>pmie</command>
on the systems where the PCP archive logs reside. The problem then reduces
to <command>pmlogger</command> deployment as discussed in <xref linkend="LE56598-TITLE">.
</para>
</listitem>
<listitem><para>As an example of the &ldquo;distributed management with
centralized control&rdquo; philosophy, designate some system to act as
the PCP Management Site to run all <command>pmlogger</command> and <command>
pmie</command> instances. This arrangement is shown in <xref linkend="LE43402-TITLE">.
</para>
</listitem></itemizedlist>
<?Pub _newpage>
<para>One <command>pmie</command> instance is capable of monitoring multiple
PCP collector systems; for example, to evaluate some universal rules that
apply to all hosts. At the same time a single PCP collector system may
be monitored by multiple <command>pmie</command> instances; for example,
for site-specific and universal rule evaluation, or to support both tactical
performance management (operations) and strategic performance management
(capacity planning). Both situations are depicted in <xref linkend="LE43402-TITLE">.
</para>
<figure width="wide">
<graphic entityref="a12228" scale="NO"></graphic>
<title id="LE43402-TITLE">PCP Management Site Deployment</title>
</figure>
</section>
<?Pub _newpage>
<section>
<title>Resource Demands for the Deployment Options</title>
<para>Depending on the complexity of the rule sets, the number of hosts
being monitored, and the evaluation frequency, <command>pmie</command>
may consume CPU cycles significantly above the resources required to simply
fetch the values of the performance metrics. If this becomes significant,
then real-time deployment of <command>pmie</command> away from the PCP
collector systems should be considered in order to avoid the &ldquo;you're
part of the problem, not the solution&rdquo; scenario in terms of CPU
utilization on a heavily loaded server.</para>
</section>
<section>
<title>Operational Management</title>
<para>An initial administrative cost is associated with configuring each <command>
pmie</command> instance, particularly in the development of the rule sets
that accurately capture and classify &ldquo;good&rdquo; versus &ldquo;bad&rdquo;
performance in your environment. These rule sets almost always involve
some site-specific knowledge, particularly in respect to the &ldquo;normal&rdquo;
levels of activity and resource consumption. The <command>pmieconf</command>
tool (see <xref linkend="Z927039566sdc">) may be used to help develop
localized rules based upon parameterized templates covering many common
performance scenarios. In complex environments, customizing these rules
may occur over an extended period and require considerable performance
analysis insight.</para>
<para>One of the functions of <command>pmie</command> provides for continual
detection of adverse performance and the automatic generation of alarms
(visible, audible, e-mail, pager, and so on). Uncontrolled deployment
of this alarm initiating capability throughout the enterprise may cause
havoc.</para>
<para>These considerations favor a centralized <command>pmie</command>
deployment at a small number of PCP monitor sites, or in a PCP Management
Site as shown in <xref linkend="LE43402-TITLE">.</para>
<para>However, it is most likely that knowledgeable users with specific
needs may find a local deployment of <command>pmie</command> most useful
to track some particular class of service difficulty or resource utilization.
In these cases, the alarm propagation is unlikely to be required or is
confined to the system on which <command>pmie</command> is running.</para>
<para>Configuration and management of a number of <command>pmie</command>
instances is made much easier with the scripts and control files described
in <xref linkend="Z927039824sdc">.</para>
</section>
</section>
</chapter>
<?Pub *0000026038>

Performance Co-Pilot

    Introduction
    Installation
        Installing Collector Hosts
        Installing Monitor Host
        Dynamic Host Configuration
        Installation Health Check
    System Level Performance Monitoring
        Monitoring Live Performance Metrics
        Retrospective Performance Analysis
        Visualizing iostat and sar Data
    Process Level Performance Monitoring
        Live and Retrospective Process Monitoring
        Application Instrumentation
    Performance Metrics Inference
    PCP Web Interfaces
        Performance Metrics Web Daemon
        Web Interface for Performance Metrics
    Customizing and Extending PCP
    Additional Information

Introduction

Performance Co-Pilot (PCP) is an open source framework and toolkit for monitoring, analyzing, and responding to details of live and historical system performance. PCP has a fully distributed, plug-in based architecture making it particularly well suited to centralized analysis of complex environments and systems. Custom performance metrics can be added using the C, C++, Perl, and Python interfaces.

This page provides quick instructions how to install and use PCP on a set of RHEL hosts of which one (a monitor host) will be used for monitoring and analyzing itself and other hosts (collector hosts).
Installation

PCP is supported on RHEL 6.6+ and RHEL 7+ and available from the EPEL repositories for earlier version.

For older releases, either enable EPEL with Yum (see this page for details - but be careful not to overwrite any RHEL packages with EPEL packages) or you can grab the latest PCP packages manually from the EPEL repositories (see http://dl.fedoraproject.org/pub/epel/).
Installing Collector Hosts

To install basic PCP tools and services and enable collecting performance data on a host, simply do:

    # yum install pcp

    # chkconfig pmcd on

    # service pmcd start

    # chkconfig pmlogger on

    # service pmlogger start

This will enable the Performance Metrics Collector Daemon (pmcd) on the host which then in turn will control and request metrics on behalf of clients from various Performance Metrics Domain Agents (PMDAs) which provide the actual data from different components in the system, for example from the Linux PMDA or NFS Client PMDA. The default configuration includes over 1000 metrics with negligible overall overhead. Local PCP archive logs will also be enabled on the host for convenience with pmlogger (RHKB 1146283 contains some additional logging related considerations).

To enable PMDAs which are not enabled by default, for example the NFS Client PMDA, run the corresponding Install script:

    # cd /var/lib/pcp/pmdas/nfsclient

    # ./Install

The client tools will contact local or remote PMCDs as needed, communication with PMCD over the network uses TCP port 44321.
Installing Monitor Host

The following additional packages will be needed on the monitoring host:

    # yum install pcp-doc pcp-gui pcp-webapi

To enable centralized archive log collection on the monitoring host, its pmlogger is configured to fetch performance metrics from collector hosts. Add each collector host to the pmlogger configuration file /etc/pcp/pmlogger/control and then restart the pmlogger service on the monitoring host:

    # echo <host1> n n PCP_LOG_DIR/pmlogger/<host1> -r -T24h10m -c config.remote >> /etc/pcp/pmlogger/control

    # service pmlogger restart

Checks for remote log collection will be done every half an hour so it might take a while before remote logs will start to appear. If you do not wish to wait, run /usr/libexec/pcp/bin/pmlogger_check -C manually.
Dynamic Host Configuration

In dynamic environments manually configuration every host is not feasible, perhaps even impossible. PCP Manager (PMMGR) can be used instead of PMLOGGER and PMIE to auto-discover and auto-configure new collector hosts. Please see the pmmgr manual page for details on this.
Installation Health Check

Basic installation health check for running services, network connectivity between hosts, and enabled PMDAs can be done simply with:

    $ pcp -h <host>

System Level Performance Monitoring

PCP comes with a wide range of command line utilities for accessing live performance metrics via PMCDs or historical data using archive logs. The following examples illustrate some of the most useful use cases, please see the corresponding manual pages for each command for additional information. In the below examples -h <host> is always optional, the default is the local host.
Monitoring Live Performance Metrics

Display all the enabled performance metrics on a host (use with -t to include a short description for each):

    $ pminfo -h <host>

Display detailed information about a performance metric and its current values:

    $ pminfo -dftT disk.partitions.read -h <host>

Monitor live disk write operations per partition with two second interval using fixed point notation (use -i instance to list only certain metrics and -r for raw values):

    $ pmval -t 2s -f 3 disk.partitions.write -h <host>

Monitor live CPU load, memory usage, and disk write operations per partition with two second interval using fixed width columns:

    $ pmdumptext -i -l 'host:kernel.all.load[1]' 'host:mem.util.used' 'host:disk.partitions.write'

Monitor system metrics in a top like window (this needs a large terminal):

    $ pmatop -h <host>

Monitor system metrics in a sar like fashion with two second interval from two different hosts:

    $ pmstat -t 2s -h <host1> -h <host2>

Monitor system metrics in an iostat like fashion with two second interval:

    $ pmiostat -t 2s -h <host>

Monitor performance metrics with a GUI application with two second default interval from two different hosts. Use File->New Chart to select metrics to be included in a new view and use File->Open View to use a predefined view:

    $ pmchart -t 2s -h <host1> -h <host2>

Retrospective Performance Analysis

PCP archive logs are located under /var/log/pcp/pmlogger/<host>, archive names indicate the date they cover.

Check the host and the time period an archive covers:

    $ pmdumplog -l <archive>

Check PCP configuration at the time when an archive was created:

    $ pcp -a <archive>

Display all enabled performance metrics at the time when an archive was created:

    $ pminfo -a <archive>

Display detailed information about a performance metric at the time when an archive was created:

    $ pminfo -df mem.freemem -a <archive>

Dump past disk write operations per partition in an archive using fixed point notation (use -i instance to list only certain metrics and -r for raw values):

    $ pmval -f 3 disk.partitions.write -a <archive>

Replay past disk write operations per partition in an archive with two second interval using fixed point notation between 9 AM and 10 AM (use full dates with syntax like @"2014-08-20 14:00:00"):

    $ pmval -d -t 2s -f 3 disk.partitions.write -S @09:00 -T @10:00 -a <archive>

Calculate average values of performance metrics in an archive between 9 AM / 10 AM using table like formatting including the time of minimum/maximum value and the actual minimum/maximum value:

    $ pmlogsummary -HlfiImM -S @09:00 -T @10:00 <archive> disk.partitions.write mem.freemem

Dump past CPU load, memory usage, and disk write operations per partition in an archive averaged over 10 minute interval with fixed columns between 9 AM and 10 AM:

    $ pmdumptext -t 10m -i -l -S @09:00 -T @10:00 'kernel.all.load[1]' 'mem.util.used' 'disk.partitions.write' -a <archive>

Summarize differences in past performance metrics between two archives, comparing 2 AM / 3 AM in the first archive to 9 AM / 10 AM in the second archive (grep for '+' to quickly see values which were zero during the first period):

    $ /usr/libexec/pcp/bin/pmwtf -S @02:00 -T @03:00 -B @09:00 -E @10:00 <archive1> <archive2>

Replay past system metrics in an archive in a top like window starting 9 AM (this needs a large window):

    $ pmatop -S @09:00 -a <archive>

Dump past system metrics in a sar like fashion averaged over 10 minute interval in an archive between 9 AM and 10 AM:

    $ pmstat -t 10m -S @09:00 -T @10:00 -a <archive>

Dump past system metrics in an iostat like fashion averaged over one hour interval in an archive:

    $ pmiostat -t 1h -a <archive>

Replay performance metrics with a GUI application with two second default interval in an archive between 9 AM and 10 AM. Use File->New Chart to select metrics to be included in a new view and use File->Open View to use a predefined view:

    $ pmchart -t 2s -S @09:00 -T @10:00 -a <archive>

Merge several archives as a new combined archive (see the manual page how to write configuration file to collect only certain metrics):

    $ pmlogextract <archive1> <archive2> <newarchive>

Visualizing iostat and sar Data

iostat and sar data can be imported as PCP archives which then allows inspecting and visualizing the data with PCP tools. To iostat2pcp importer is in the pcp-import-iostat2pcp package and the sar2pcp importer is in the pcp-import-sar2pcp package.

Import iostat data to a new PCP archive and visualize it:

    $ iostat -t -x 2 > iostat.out

    $ iostat2pcp iostat.out iostat.pcp

    $ pmchart -t 2s -a iostat.pcp

Import sar data from an existing sar archive to a new PCP archive and visualize it:

    $ sar2pcp /var/log/sa/sa15 sar.pcp

    $ pmchart -2 2s -a sar.pcp

Process Level Performance Monitoring

PCP provides details of each running process via the standard PCP interfaces and tools on the localhost but due to security and performance reasons most of the process related information is not stored in archive logs. Custom application instrumentation is possible with the Memory Mapped Value (MMV) PMDA.
Live and Retrospective Process Monitoring

Display all the available process related metrics:

    $ pminfo proc

Monitor the number of open file descriptors of the process 1234:

    $ pmval -t 2s 'proc.fd.count[1234]'

Monitor the CPU time, memory usage (RSS), and the number of threads of the process 1234 (--host local: is a workaround needed for the time being):

    $ pmdumptext --host local: -t 2s 'proc.psinfo.utime[1234]' 'proc.memory.rss[1234]' 'proc.psinfo.threads[1234]'

Display all the available process related metrics in an archive:

    $ pminfo proc -a <archive>

Display the number of running processes on 2014-08-20 14:00:

    $ pmval -s 1 -S @"2014-08-20 14:00" proc.nprocs -a <archive>

Application Instrumentation

Applications can be instrumented in the PCP world by using Memory Mapped Values (MMVs). pmdammv is a PMDA which exports application level performance metrics using memory mapped files. It offers an extremely low overhead instrumentation facility that is well-suited to long running, mission critical applications where it is desirable to have performance metrics and availability information permanently enabled.

Application to be instrumented with MMV need to be PCP MMV aware, APIs are available for several languages including C, C++, Perl, and Python. Java applications may use the separate Parfait class library for enabling MMV.

Instrumentation of unaltered Java applications is a known feature request and there has been some discussion on implementation strategy but currently there is not active development ongoing yet on this front.

See the Performance Co-Pilot Programmer's Guide for more information about application instrumentation.
Performance Metrics Inference

Performance Metrics Inference Engine (PMIE) can evaluate rules and generate alarms, run scripts, or automate system management tasks based on live or past performance metrics.

To enable PMIE on a host, just enable and start the service:

    # chkconfig pmie on

    # service pmie start

To enable the monitoring host to run PMIE for collector hosts, add each host to the /etc/pcp/pmie/control configuration file:

    # echo <host1> n PCP_LOG_DIR/pmie/<host1> -c config.remote

    # service pmie restart

Some examples in plain English what could be done with PMIE:

    If the number of IP received packets exceeds a threshold run a script to adjust firewall rules to limit the incoming traffic
    If 3 out of 4 consecutive samples taken every minute of disk operations exceeds a threshold between 9 AM and 5 PM send an email and write a system log message
    If all hosts in a group have CPU load over a threshold for more than 10 minutes or they have more application processes running than a threshold limit generate an alarm and run a script to tune the application

The following shows a simple PMIE script, checks its syntax, runs it against an archive, and prints a simple message if more than 5 GB of memory was in use between 9 AM and 10 AM using one minute sampling interval:

    $ cat pmie.ex

    bloated = (  mem.util.used > 5 Gbyte )

      -> print "%v memory used on %h!"

    $ pmie -C pmie.ex

    $ pmie -t 1m -c pmie.ex -S @09:00 -T @10:00 -a <archive>

PCP Web Interfaces

PCP offers web interfaces for users and developers to interact with performance data.
Performance Metrics Web Daemon

Performance Metrics Web Daemon (PMWEBD) is a front-end to both PMCD and PCP archives, providing a JSON interface suitable for use by web-based tools wishing to access performance data over HTTP. Custom applications can access all the available PCP information using this method, including possible data generated by custom PMDAs.
Web Interface for Performance Metrics

A modern web interface for accessing PCP performance metrics is currently a work in progress in PCP upstream and it should be available in not too distant future as part of PCP.

The upstream work is described at a recent blog entry, the used web interface is Grafana.

More details will be added here once this feature is available for end-users.
Customizing and Extending PCP

As explained above, PCP PMDAs offer a way for administrators and developers to customize and extend the default PCP installation. The pcp-libs-devel package contains all the needed development related examples, headers, and libraries. Existing PMDA can be easily enabled, below is a quick list of references for starting with development of a new PDMA:

    Under /var/lib/pcp/pmdas/ the simple, sample, and txmon PMDAs are easy to read PMDAs
    A simple command line client is /usr/share/pcp/demos/pmclient, good Python examples are /usr/libexec/pcp/bin/pcp/pcp-*, and a slightly more complex examples are the pmiostat, pmatop, pmcollect commands
    The examples in the PCP Git tree under src/pmwebapi/jsdemos are helpful when developing a web application

Additional Information

    PCP home page
    Performance Co-Pilot in Wikipedia
    http://www.pcp.io/presentations.html - PCP Presentations
    http://www.pcp.io/doc/pcp-users-and-administrators-guide.pdf - Performance Co-Pilot User's and Administrator's Guide
    http://www.pcp.io/doc/pcp-programmers-guide.pdf - Performance Co-Pilot Programmer's Guide
    http://flocktofedora.org/wp-content/uploads/2013/08/flock-2013-perftools.pdf - PCP + Systemtap: Performance Monitoring for Workstations and Networks

